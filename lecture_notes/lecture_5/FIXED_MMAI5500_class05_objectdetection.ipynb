{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVWNmqs3ZFKx"
   },
   "source": [
    "# Object detection with ImageAI\n",
    "\n",
    "In this tutorial we will be using the [ImageAI library](https://github.com/OlafenwaMoses/ImageAI).\n",
    "It provides classes and pre-trained models for both object detection and image recognition, as well as, the ability to train your custom models.\n",
    "\n",
    "## Install ImageAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK6_KN_aZUrs",
    "outputId": "c45db725-7e4e-40b7-cffb-978b30d4a21e"
   },
   "outputs": [],
   "source": [
    "# !pip install -U imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Large files were not included in Repo due to large file sizes in Github.\n",
    "# Please make sure to download from the wget commands below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61VILWtKfjyi"
   },
   "source": [
    "## Upload the models to Colab\n",
    "\n",
    "Download them to your local computer and then upload to Colab.\n",
    "\n",
    " * [YOLO v3](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt)\n",
    " * [Tiny YOLO v3](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt)\n",
    " * [RetinaNet](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth)\n",
    "\n",
    "\n",
    "```Python\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "```\n",
    "\n",
    "## Set runtime to GPU\n",
    "\n",
    " Runtime > Change runtime type > Hardware acceleratior > GPU\n",
    "\n",
    "## The data\n",
    "\n",
    "Upload `tutorial_images.zip` and upzip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WuoG8KWBwYtg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google Collab code to download all the files.\n",
    "# # Download retinanet\n",
    "# !wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth\n",
    "\n",
    "# # Download YOLOv3\n",
    "# !wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt\n",
    "\n",
    "# # Download Tiny YOLOv3\n",
    "# !wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hByLvdeAfmM7",
    "outputId": "4a877a7b-1a9a-4f5a-da0d-f5800e4f9411"
   },
   "outputs": [],
   "source": [
    "# Since the files are already set up in my directory, I will skip this.\n",
    "\n",
    "# !mkdir -p data/images\n",
    "# !unzip tutorial_images.zip -d data/images\n",
    "# image_path = 'data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imageai\n",
    "# pip install opencv-python\n",
    "# pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zg0_Q_g7YxBk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsJ4uEPBaL-L"
   },
   "source": [
    "Create a new instance of the `ObjectDetection` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JIwDGqn-ZXqr"
   },
   "outputs": [],
   "source": [
    "detector = ObjectDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVMx528vaTww"
   },
   "source": [
    "Set the model type of the object detection instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BOY_6JzWZpQ6"
   },
   "outputs": [],
   "source": [
    "# YOLOv3\n",
    "detector.setModelTypeAsYOLOv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuhRU6LPac6u"
   },
   "source": [
    "Set the path to the downloaded, pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0kU4315Kad5t"
   },
   "outputs": [],
   "source": [
    "# YOLOv3\n",
    "detector.setModelPath(\"yolov3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clJQrkC_aqb6"
   },
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNFKmMrSZ6nA",
    "outputId": "ec699178-214a-487c-9bd4-e40a0e8c707f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eksdxTbHfxRJ"
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Kn3sJ7diasYm"
   },
   "outputs": [],
   "source": [
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\", \"11.jpg\"),\n",
    "                                             output_image_path=\"11_detected.jpg\",\n",
    "                                             minimum_percentage_probability=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vntofcc2f984",
    "outputId": "bb112eb7-9614-4cf8-8ad9-82ff0dc1fb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  :  99.99  :  [322, 37, 548, 359]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [99, 52, 302, 362]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [577, 72, 798, 457]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [0, 91, 280, 525]\n",
      "--------------------------------\n",
      "cup  :  99.84  :  [565, 403, 627, 486]\n",
      "--------------------------------\n",
      "cup  :  99.76  :  [466, 321, 508, 380]\n",
      "--------------------------------\n",
      "cup  :  99.66  :  [334, 390, 384, 458]\n",
      "--------------------------------\n",
      "cup  :  97.75  :  [507, 339, 543, 397]\n",
      "--------------------------------\n",
      "laptop  :  99.99  :  [169, 256, 418, 413]\n",
      "--------------------------------\n",
      "cell phone  :  99.28  :  [507, 399, 562, 422]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid path, path not pointing to a valid file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# YOLOV3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetModelPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/retinanet_resnet50_fpn_coco-eeacb38b.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:210\u001b[0m, in \u001b[0;36mObjectDetection.setModelPath\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__model_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid path, path not pointing to a valid file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid path, path not pointing to a valid file."
     ]
    }
   ],
   "source": [
    "# YOLOV3\n",
    "detector.setModelPath(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/retinanet_resnet50_fpn_coco-eeacb38b.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=None`.\n",
      "  warnings.warn(msg)\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/retinanet_resnet50_fpn_coco-eeacb38b.pth\")\n",
    "detector.loadModel()\n",
    "\n",
    "# detections = detector.detectObjectsFromImage(input_image =os_path.join())\n",
    "\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\", \"11.jpg\"),\n",
    "                                             output_image_path=\"11_detected.jpg\",\n",
    "                                             minimum_percentage_probability=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrAiGyh8gagA"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Test the performance on all images in `tutorial_images` using `YOLOv3` and `minimum_percentage_probability = 30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erY58_DK2RAi"
   },
   "outputs": [],
   "source": [
    "# Did this above\n",
    "my_images = [\"6.jpg\", \"7.jpg\", \"8.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection()\n",
    "\n",
    "# YOLOv3\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"yolov3.pt\")\n",
    "detector.loadModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/01.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/03.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/02.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/12.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/06.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/07.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/13.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/05.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/11.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/10.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/04.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/09.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/08.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "from glob import glob\n",
    "fnames = glob(image_path +\"/*.jpg\")\n",
    "print(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  :  100.0  :  [174, 0, 669, 431]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [93, 10, 333, 396]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [476, 41, 703, 373]\n",
      "--------------------------------\n",
      "person  :  99.94  :  [351, 85, 379, 171]\n",
      "--------------------------------\n",
      "person  :  99.52  :  [1, 78, 31, 247]\n",
      "--------------------------------\n",
      "person  :  98.75  :  [80, 82, 125, 233]\n",
      "--------------------------------\n",
      "person  :  99.95  :  [372, 81, 399, 159]\n",
      "--------------------------------\n",
      "bicycle  :  99.81  :  [401, 135, 712, 421]\n",
      "--------------------------------\n",
      "bicycle  :  99.83  :  [67, 145, 320, 421]\n",
      "--------------------------------\n",
      "car  :  95.39  :  [455, 91, 531, 160]\n",
      "--------------------------------\n",
      "traffic light  :  99.97  :  [658, 10, 685, 24]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [282, 5, 442, 167]\n",
      "--------------------------------\n",
      "person  :  99.94  :  [281, 49, 335, 150]\n",
      "--------------------------------\n",
      "bottle  :  43.97  :  [227, 130, 241, 151]\n",
      "--------------------------------\n",
      "chair  :  99.73  :  [4, 157, 117, 316]\n",
      "--------------------------------\n",
      "chair  :  95.45  :  [107, 219, 282, 315]\n",
      "--------------------------------\n",
      "chair  :  70.41  :  [251, 100, 289, 134]\n",
      "--------------------------------\n",
      "chair  :  98.01  :  [162, 102, 203, 140]\n",
      "--------------------------------\n",
      "tvmonitor  :  99.53  :  [461, 49, 514, 96]\n",
      "--------------------------------\n",
      "tvmonitor  :  63.1  :  [270, 76, 296, 98]\n",
      "--------------------------------\n",
      "laptop  :  99.31  :  [154, 152, 266, 245]\n",
      "--------------------------------\n",
      "laptop  :  99.18  :  [81, 125, 185, 201]\n",
      "--------------------------------\n",
      "laptop  :  99.49  :  [484, 164, 589, 226]\n",
      "--------------------------------\n",
      "laptop  :  99.59  :  [306, 206, 445, 312]\n",
      "--------------------------------\n",
      "laptop  :  99.79  :  [410, 149, 495, 202]\n",
      "--------------------------------\n",
      "laptop  :  99.65  :  [214, 192, 360, 290]\n",
      "--------------------------------\n",
      "laptop  :  99.61  :  [287, 129, 344, 163]\n",
      "--------------------------------\n",
      "laptop  :  96.71  :  [63, 117, 136, 175]\n",
      "--------------------------------\n",
      "laptop  :  97.31  :  [233, 116, 288, 161]\n",
      "--------------------------------\n",
      "laptop  :  98.66  :  [357, 146, 414, 186]\n",
      "--------------------------------\n",
      "mouse  :  87.47  :  [402, 193, 432, 208]\n",
      "--------------------------------\n",
      "keyboard  :  99.51  :  [57, 155, 112, 169]\n",
      "--------------------------------\n",
      "keyboard  :  99.93  :  [239, 242, 308, 270]\n",
      "--------------------------------\n",
      "cell phone  :  63.54  :  [205, 231, 244, 248]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [322, 37, 548, 359]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [99, 52, 302, 362]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [577, 72, 798, 457]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [0, 91, 280, 525]\n",
      "--------------------------------\n",
      "cup  :  99.84  :  [565, 403, 627, 486]\n",
      "--------------------------------\n",
      "cup  :  99.76  :  [466, 321, 508, 380]\n",
      "--------------------------------\n",
      "cup  :  99.66  :  [334, 390, 384, 458]\n",
      "--------------------------------\n",
      "cup  :  97.75  :  [507, 339, 543, 397]\n",
      "--------------------------------\n",
      "laptop  :  99.99  :  [169, 256, 418, 413]\n",
      "--------------------------------\n",
      "cell phone  :  99.28  :  [507, 399, 562, 422]\n",
      "--------------------------------\n",
      "dog  :  99.19  :  [119, 107, 310, 363]\n",
      "--------------------------------\n",
      "dog  :  99.93  :  [338, 38, 490, 381]\n",
      "--------------------------------\n",
      "dog  :  99.81  :  [505, 132, 639, 366]\n",
      "--------------------------------\n",
      "person  :  89.07  :  [586, 418, 604, 460]\n",
      "--------------------------------\n",
      "cup  :  93.18  :  [272, 107, 307, 127]\n",
      "--------------------------------\n",
      "pottedplant  :  100.0  :  [29, 13, 135, 127]\n",
      "--------------------------------\n",
      "laptop  :  99.96  :  [138, 54, 253, 137]\n",
      "--------------------------------\n",
      "keyboard  :  99.63  :  [155, 115, 257, 135]\n",
      "--------------------------------\n",
      "book  :  98.87  :  [37, 119, 125, 138]\n",
      "--------------------------------\n",
      "vase  :  95.85  :  [58, 77, 111, 123]\n",
      "--------------------------------\n",
      "truck  :  99.98  :  [60, 102, 977, 550]\n",
      "--------------------------------\n",
      "motorbike  :  99.89  :  [68, 24, 711, 567]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [369, 190, 388, 235]\n",
      "--------------------------------\n",
      "person  :  99.95  :  [350, 195, 368, 233]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [333, 190, 348, 233]\n",
      "--------------------------------\n",
      "bench  :  99.79  :  [23, 177, 223, 368]\n",
      "--------------------------------\n",
      "bench  :  99.71  :  [179, 183, 267, 295]\n",
      "--------------------------------\n",
      "bird  :  99.88  :  [105, 63, 249, 349]\n",
      "--------------------------------\n",
      "bird  :  99.96  :  [265, 91, 442, 353]\n",
      "--------------------------------\n",
      "bird  :  99.9  :  [200, 110, 302, 302]\n",
      "--------------------------------\n",
      "bird  :  96.3  :  [1, 123, 73, 332]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "output_folder= \"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/output_path\"\n",
    "i = 0\n",
    "\n",
    "for item in fnames:\n",
    "    i = i + 1\n",
    "    output_image_path = os.path.join(output_folder, f\"output_image_{i}.jpg\")\n",
    "    detections = detector.detectObjectsFromImage(input_image=item,\n",
    "                                             output_image_path= output_image_path,\n",
    "                                             minimum_percentage_probability=30)\n",
    "    # Results for each image:\n",
    "    \n",
    "    \n",
    "    for eachObject in detections:\n",
    "        print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghJ6oHyVdIKp"
   },
   "source": [
    "**Tutorial question 1**: How many objects were detected in images 6, 7 and 9?\n",
    "\n",
    "**Tutorial question 2**: What is detected in image 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFr444wSgSgw"
   },
   "outputs": [],
   "source": [
    "# Objects identifed = 1, 1 , 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7oxd8vigrpa"
   },
   "source": [
    "\n",
    "## Task 2\n",
    "Compare the performance of the 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "yTfEHc9JfqIR",
    "outputId": "ac1b3485-248b-4c32-a66a-fe28a3152e86"
   },
   "outputs": [],
   "source": [
    "# Put in a for-loop\n",
    "\n",
    "# Find a list of images, feed into the image one by one, detected with better name\n",
    "# Output will be 1 detected, image 2 will be 2 detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wCZuUocbubg",
    "outputId": "8ef324af-6efb-4824-da8d-e4af6ed563f9"
   },
   "outputs": [],
   "source": [
    "# Which model is best? Any of them, really. But we saw in the Waterfall img that it accidentally deleted a ledge as a person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN-6FQ3ri8NC"
   },
   "source": [
    "**Tutorial question 3**: Which model was best?\n",
    "\n",
    "## Object detection in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZVsKaeWaOaH",
    "outputId": "e405051c-7374-4636-b533-196590e7679a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  tutorial_video.zip\n",
      "  inflating: data/video/traffic.mp4  \n",
      "  inflating: data/video/traffic-mini.mp4  \n",
      "  inflating: data/video/holo1.mp4    \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/video\n",
    "!unzip tutorial_video.zip -d data/video\n",
    "video_path = 'data/video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6gRGb3pojHIo"
   },
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "detector = VideoObjectDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "D4uXa6O1jKYY"
   },
   "outputs": [],
   "source": [
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"yolov3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XJ_jHJkDjOi5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8odx7O9jQRw",
    "outputId": "6cda5308-2466-4106-9efe-d1a3fb979bc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame :  1\n",
      "Processing Frame :  2\n",
      "Processing Frame :  3\n",
      "Processing Frame :  4\n",
      "Processing Frame :  5\n",
      "Processing Frame :  6\n",
      "Processing Frame :  7\n",
      "Processing Frame :  8\n",
      "Processing Frame :  9\n",
      "Processing Frame :  10\n",
      "Processing Frame :  11\n",
      "Processing Frame :  12\n",
      "Processing Frame :  13\n",
      "Processing Frame :  14\n",
      "Processing Frame :  15\n",
      "Processing Frame :  16\n",
      "Processing Frame :  17\n",
      "Processing Frame :  18\n",
      "Processing Frame :  19\n",
      "Processing Frame :  20\n",
      "Processing Frame :  21\n",
      "Processing Frame :  22\n",
      "Processing Frame :  23\n",
      "Processing Frame :  24\n",
      "Processing Frame :  25\n",
      "Processing Frame :  26\n",
      "Processing Frame :  27\n",
      "Processing Frame :  28\n",
      "Processing Frame :  29\n",
      "Processing Frame :  30\n",
      "Processing Frame :  31\n",
      "Processing Frame :  32\n",
      "Processing Frame :  33\n",
      "Processing Frame :  34\n",
      "Processing Frame :  35\n",
      "Processing Frame :  36\n",
      "Processing Frame :  37\n",
      "Processing Frame :  38\n",
      "Processing Frame :  39\n",
      "Processing Frame :  40\n",
      "Processing Frame :  41\n",
      "Processing Frame :  42\n",
      "Processing Frame :  43\n",
      "Processing Frame :  44\n",
      "Processing Frame :  45\n",
      "Processing Frame :  46\n",
      "Processing Frame :  47\n",
      "Processing Frame :  48\n",
      "Processing Frame :  49\n",
      "Processing Frame :  50\n",
      "Processing Frame :  51\n",
      "Processing Frame :  52\n",
      "Processing Frame :  53\n",
      "Processing Frame :  54\n",
      "Processing Frame :  55\n",
      "Processing Frame :  56\n",
      "Processing Frame :  57\n",
      "Processing Frame :  58\n",
      "Processing Frame :  59\n",
      "Processing Frame :  60\n",
      "Processing Frame :  61\n",
      "Processing Frame :  62\n",
      "Processing Frame :  63\n",
      "Processing Frame :  64\n",
      "Processing Frame :  65\n",
      "Processing Frame :  66\n",
      "Processing Frame :  67\n",
      "Processing Frame :  68\n",
      "Processing Frame :  69\n",
      "Processing Frame :  70\n",
      "Processing Frame :  71\n",
      "Processing Frame :  72\n",
      "Processing Frame :  73\n",
      "Processing Frame :  74\n",
      "Processing Frame :  75\n",
      "Processing Frame :  76\n",
      "Processing Frame :  77\n",
      "Processing Frame :  78\n",
      "Processing Frame :  79\n",
      "Processing Frame :  80\n",
      "Processing Frame :  81\n",
      "Processing Frame :  82\n",
      "Processing Frame :  83\n",
      "Processing Frame :  84\n",
      "Processing Frame :  85\n",
      "Processing Frame :  86\n",
      "Processing Frame :  87\n",
      "Processing Frame :  88\n",
      "Processing Frame :  89\n",
      "Processing Frame :  90\n",
      "Processing Frame :  91\n",
      "Processing Frame :  92\n",
      "Processing Frame :  93\n",
      "Processing Frame :  94\n",
      "Processing Frame :  95\n",
      "Processing Frame :  96\n",
      "Processing Frame :  97\n",
      "Processing Frame :  98\n",
      "Processing Frame :  99\n",
      "Processing Frame :  100\n",
      "Processing Frame :  101\n",
      "Processing Frame :  102\n",
      "Processing Frame :  103\n",
      "Processing Frame :  104\n",
      "Processing Frame :  105\n",
      "Processing Frame :  106\n",
      "Processing Frame :  107\n",
      "Processing Frame :  108\n",
      "Processing Frame :  109\n",
      "Processing Frame :  110\n",
      "Processing Frame :  111\n",
      "Processing Frame :  112\n",
      "Processing Frame :  113\n",
      "Processing Frame :  114\n",
      "Processing Frame :  115\n",
      "Processing Frame :  116\n",
      "Processing Frame :  117\n",
      "Processing Frame :  118\n",
      "Processing Frame :  119\n",
      "Processing Frame :  120\n",
      "Processing Frame :  121\n",
      "Processing Frame :  122\n",
      "Processing Frame :  123\n",
      "Processing Frame :  124\n",
      "Processing Frame :  125\n",
      "Processing Frame :  126\n",
      "Processing Frame :  127\n",
      "Processing Frame :  128\n",
      "Processing Frame :  129\n",
      "Processing Frame :  130\n",
      "Processing Frame :  131\n",
      "Processing Frame :  132\n",
      "Processing Frame :  133\n",
      "Processing Frame :  134\n",
      "Processing Frame :  135\n",
      "Processing Frame :  136\n",
      "Processing Frame :  137\n",
      "Processing Frame :  138\n",
      "Processing Frame :  139\n",
      "Processing Frame :  140\n",
      "Processing Frame :  141\n",
      "Processing Frame :  142\n",
      "Processing Frame :  143\n",
      "Processing Frame :  144\n",
      "Processing Frame :  145\n",
      "Processing Frame :  146\n",
      "Processing Frame :  147\n",
      "Processing Frame :  148\n",
      "Processing Frame :  149\n",
      "Processing Frame :  150\n",
      "Processing Frame :  151\n",
      "Processing Frame :  152\n",
      "Processing Frame :  153\n",
      "Processing Frame :  154\n",
      "Processing Frame :  155\n",
      "Processing Frame :  156\n",
      "Processing Frame :  157\n",
      "Processing Frame :  158\n",
      "Processing Frame :  159\n",
      "Processing Frame :  160\n",
      "Processing Frame :  161\n",
      "Processing Frame :  162\n",
      "Processing Frame :  163\n",
      "Processing Frame :  164\n",
      "Processing Frame :  165\n",
      "Processing Frame :  166\n",
      "Processing Frame :  167\n",
      "Processing Frame :  168\n",
      "Processing Frame :  169\n",
      "Processing Frame :  170\n",
      "Processing Frame :  171\n",
      "Processing Frame :  172\n",
      "Processing Frame :  173\n",
      "Processing Frame :  174\n",
      "Processing Frame :  175\n",
      "Processing Frame :  176\n",
      "Processing Frame :  177\n",
      "Processing Frame :  178\n",
      "Processing Frame :  179\n",
      "traffic-mini_detected.mp4\n"
     ]
    }
   ],
   "source": [
    "video_path = detector.detectObjectsFromVideo(os.path.join(video_path, \"traffic-mini.mp4\"),\n",
    "                                             output_file_path=\"traffic-mini_detected\",\n",
    "                                             frames_per_second=5, log_progress=True)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj_azW8zjs_5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
