{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVWNmqs3ZFKx"
   },
   "source": [
    "# Object detection with ImageAI\n",
    "\n",
    "In this tutorial we will be using the [ImageAI library](https://github.com/OlafenwaMoses/ImageAI).\n",
    "It provides classes and pre-trained models for both object detection and image recognition, as well as, the ability to train your custom models.\n",
    "\n",
    "## Install ImageAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK6_KN_aZUrs",
    "outputId": "c45db725-7e4e-40b7-cffb-978b30d4a21e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageai in /Users/christine/anaconda3/lib/python3.11/site-packages (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Large files were not included in Repo due to large file sizes in Github.\n",
    "# Please make sure to download from the wget commands below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61VILWtKfjyi"
   },
   "source": [
    "## Upload the models to Colab\n",
    "\n",
    "Download them to your local computer and then upload to Colab.\n",
    "\n",
    " * [YOLO v3](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt)\n",
    " * [Tiny YOLO v3](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt)\n",
    " * [RetinaNet](https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth)\n",
    "\n",
    "\n",
    "```Python\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "```\n",
    "\n",
    "## Set runtime to GPU\n",
    "\n",
    " Runtime > Change runtime type > Hardware acceleratior > GPU\n",
    "\n",
    "## The data\n",
    "\n",
    "Upload `tutorial_images.zip` and upzip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuoG8KWBwYtg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lOwOVmV0YjS",
    "outputId": "856c4c55-41b8-4786-def6-174bb3e32c27"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2240550236.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Download retinanet\n",
    "!wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/retinanet_resnet50_fpn_coco-eeacb38b.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tmNHGTKzXAo",
    "outputId": "802fedce-8d75-4a6c-9ef0-6b3f8634cc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# Download YOLOv3\n",
    "!wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3G6cCPAcz5Kc",
    "outputId": "36586f6b-fda4-4767-9642-1f0473c6e981"
   },
   "outputs": [],
   "source": [
    "# Download Tiny YOLOv3\n",
    "!wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/tiny-yolov3.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hByLvdeAfmM7",
    "outputId": "4a877a7b-1a9a-4f5a-da0d-f5800e4f9411"
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/images\n",
    "!unzip tutorial_images.zip -d data/images\n",
    "image_path = 'data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zg0_Q_g7YxBk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install imageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/66/82/564168a349148298aca281e342551404ef5521f33fba17b388ead0a84dc5/opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/christine/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/1f/34/c93873c37f93154d982172755f7e504fdbae6c760499303a3111ce6ce327/torch-2.4.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached torch-2.4.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/66/f6/a2f07a3f5385b37c45b8e14448b8610a8618dfad18ea437cb23b4edc50c5/torchvision-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: filelock in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/christine/anaconda3/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/christine/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/christine/anaconda3/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christine/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/christine/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.1-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "Successfully installed torch-2.4.1 torchvision-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageai\n",
      "  Downloading imageai-3.0.3-py3-none-any.whl.metadata (340 bytes)\n",
      "Downloading imageai-3.0.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageai\n",
      "Successfully installed imageai-3.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install imageai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsJ4uEPBaL-L"
   },
   "source": [
    "Create a new instance of the `ObjectDetection` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JIwDGqn-ZXqr"
   },
   "outputs": [],
   "source": [
    "detector = ObjectDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVMx528vaTww"
   },
   "source": [
    "Set the model type of the object detection instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BOY_6JzWZpQ6"
   },
   "outputs": [],
   "source": [
    "# YOLOv3\n",
    "detector.setModelTypeAsYOLOv3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuhRU6LPac6u"
   },
   "source": [
    "Set the path to the downloaded, pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0kU4315Kad5t"
   },
   "outputs": [],
   "source": [
    "# YOLOv3\n",
    "detector.setModelPath(\"yolov3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clJQrkC_aqb6"
   },
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNFKmMrSZ6nA",
    "outputId": "ec699178-214a-487c-9bd4-e40a0e8c707f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eksdxTbHfxRJ"
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kn3sJ7diasYm"
   },
   "outputs": [],
   "source": [
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\", \"11.jpg\"),\n",
    "                                             output_image_path=\"11_detected.jpg\",\n",
    "                                             minimum_percentage_probability=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vntofcc2f984",
    "outputId": "bb112eb7-9614-4cf8-8ad9-82ff0dc1fb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  :  99.99  :  [322, 37, 548, 359]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [99, 52, 302, 362]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [577, 72, 798, 457]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [0, 91, 280, 525]\n",
      "--------------------------------\n",
      "cup  :  99.84  :  [565, 403, 627, 486]\n",
      "--------------------------------\n",
      "cup  :  99.76  :  [466, 321, 508, 380]\n",
      "--------------------------------\n",
      "cup  :  99.66  :  [334, 390, 384, 458]\n",
      "--------------------------------\n",
      "cup  :  97.75  :  [507, 339, 543, 397]\n",
      "--------------------------------\n",
      "laptop  :  99.99  :  [169, 256, 418, 413]\n",
      "--------------------------------\n",
      "cell phone  :  99.28  :  [507, 399, 562, 422]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOV3\n",
    "detector.setModelPath(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/retinanet_resnet50_fpn_coco-eeacb38b.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=None`.\n",
      "  warnings.warn(msg)\n",
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Code to do this in RetinaNet\n",
=======
    "#\n",
>>>>>>> d98b390 (Added code.)
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/retinanet_resnet50_fpn_coco-eeacb38b.pth\")\n",
    "detector.loadModel()\n",
    "\n",
    "# detections = detector.detectObjectsFromImage(input_image =os_path.join())\n",
    "\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(\"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\", \"11.jpg\"),\n",
    "                                             output_image_path=\"11_detected.jpg\",\n",
    "                                             minimum_percentage_probability=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrAiGyh8gagA"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Test the performance on all images in `tutorial_images` using `YOLOv3` and `minimum_percentage_probability = 30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erY58_DK2RAi"
   },
   "outputs": [],
   "source": [
    "# Did this above\n",
    "my_images = [\"6.jpg\", \"7.jpg\", \"8.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection()\n",
    "\n",
    "# YOLOv3\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"yolov3.pt\")\n",
    "detector.loadModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/01.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/03.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/02.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/12.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/06.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/07.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/13.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/05.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/11.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/10.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/04.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/09.jpg', '/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/data/images/08.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "from glob import glob\n",
    "fnames = glob(image_path +\"/*.jpg\")\n",
    "print(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  :  100.0  :  [174, 0, 669, 431]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [93, 10, 333, 396]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [476, 41, 703, 373]\n",
      "--------------------------------\n",
      "person  :  99.94  :  [351, 85, 379, 171]\n",
      "--------------------------------\n",
      "person  :  99.52  :  [1, 78, 31, 247]\n",
      "--------------------------------\n",
      "person  :  98.75  :  [80, 82, 125, 233]\n",
      "--------------------------------\n",
      "person  :  99.95  :  [372, 81, 399, 159]\n",
      "--------------------------------\n",
      "bicycle  :  99.81  :  [401, 135, 712, 421]\n",
      "--------------------------------\n",
      "bicycle  :  99.83  :  [67, 145, 320, 421]\n",
      "--------------------------------\n",
      "car  :  95.39  :  [455, 91, 531, 160]\n",
      "--------------------------------\n",
      "traffic light  :  99.97  :  [658, 10, 685, 24]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [282, 5, 442, 167]\n",
      "--------------------------------\n",
      "person  :  99.94  :  [281, 49, 335, 150]\n",
      "--------------------------------\n",
      "bottle  :  43.97  :  [227, 130, 241, 151]\n",
      "--------------------------------\n",
      "chair  :  99.73  :  [4, 157, 117, 316]\n",
      "--------------------------------\n",
      "chair  :  95.45  :  [107, 219, 282, 315]\n",
      "--------------------------------\n",
      "chair  :  70.41  :  [251, 100, 289, 134]\n",
      "--------------------------------\n",
      "chair  :  98.01  :  [162, 102, 203, 140]\n",
      "--------------------------------\n",
      "tvmonitor  :  99.53  :  [461, 49, 514, 96]\n",
      "--------------------------------\n",
      "tvmonitor  :  63.1  :  [270, 76, 296, 98]\n",
      "--------------------------------\n",
      "laptop  :  99.31  :  [154, 152, 266, 245]\n",
      "--------------------------------\n",
      "laptop  :  99.18  :  [81, 125, 185, 201]\n",
      "--------------------------------\n",
      "laptop  :  99.49  :  [484, 164, 589, 226]\n",
      "--------------------------------\n",
      "laptop  :  99.59  :  [306, 206, 445, 312]\n",
      "--------------------------------\n",
      "laptop  :  99.79  :  [410, 149, 495, 202]\n",
      "--------------------------------\n",
      "laptop  :  99.65  :  [214, 192, 360, 290]\n",
      "--------------------------------\n",
      "laptop  :  99.61  :  [287, 129, 344, 163]\n",
      "--------------------------------\n",
      "laptop  :  96.71  :  [63, 117, 136, 175]\n",
      "--------------------------------\n",
      "laptop  :  97.31  :  [233, 116, 288, 161]\n",
      "--------------------------------\n",
      "laptop  :  98.66  :  [357, 146, 414, 186]\n",
      "--------------------------------\n",
      "mouse  :  87.47  :  [402, 193, 432, 208]\n",
      "--------------------------------\n",
      "keyboard  :  99.51  :  [57, 155, 112, 169]\n",
      "--------------------------------\n",
      "keyboard  :  99.93  :  [239, 242, 308, 270]\n",
      "--------------------------------\n",
      "cell phone  :  63.54  :  [205, 231, 244, 248]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [322, 37, 548, 359]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [99, 52, 302, 362]\n",
      "--------------------------------\n",
      "person  :  100.0  :  [577, 72, 798, 457]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [0, 91, 280, 525]\n",
      "--------------------------------\n",
      "cup  :  99.84  :  [565, 403, 627, 486]\n",
      "--------------------------------\n",
      "cup  :  99.76  :  [466, 321, 508, 380]\n",
      "--------------------------------\n",
      "cup  :  99.66  :  [334, 390, 384, 458]\n",
      "--------------------------------\n",
      "cup  :  97.75  :  [507, 339, 543, 397]\n",
      "--------------------------------\n",
      "laptop  :  99.99  :  [169, 256, 418, 413]\n",
      "--------------------------------\n",
      "cell phone  :  99.28  :  [507, 399, 562, 422]\n",
      "--------------------------------\n",
      "dog  :  99.19  :  [119, 107, 310, 363]\n",
      "--------------------------------\n",
      "dog  :  99.93  :  [338, 38, 490, 381]\n",
      "--------------------------------\n",
      "dog  :  99.81  :  [505, 132, 639, 366]\n",
      "--------------------------------\n",
      "person  :  89.07  :  [586, 418, 604, 460]\n",
      "--------------------------------\n",
      "cup  :  93.18  :  [272, 107, 307, 127]\n",
      "--------------------------------\n",
      "pottedplant  :  100.0  :  [29, 13, 135, 127]\n",
      "--------------------------------\n",
      "laptop  :  99.96  :  [138, 54, 253, 137]\n",
      "--------------------------------\n",
      "keyboard  :  99.63  :  [155, 115, 257, 135]\n",
      "--------------------------------\n",
      "book  :  98.87  :  [37, 119, 125, 138]\n",
      "--------------------------------\n",
      "vase  :  95.85  :  [58, 77, 111, 123]\n",
      "--------------------------------\n",
      "truck  :  99.98  :  [60, 102, 977, 550]\n",
      "--------------------------------\n",
      "motorbike  :  99.89  :  [68, 24, 711, 567]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [369, 190, 388, 235]\n",
      "--------------------------------\n",
      "person  :  99.95  :  [350, 195, 368, 233]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [333, 190, 348, 233]\n",
      "--------------------------------\n",
      "bench  :  99.79  :  [23, 177, 223, 368]\n",
      "--------------------------------\n",
      "bench  :  99.71  :  [179, 183, 267, 295]\n",
      "--------------------------------\n",
      "bird  :  99.88  :  [105, 63, 249, 349]\n",
      "--------------------------------\n",
      "bird  :  99.96  :  [265, 91, 442, 353]\n",
      "--------------------------------\n",
      "bird  :  99.9  :  [200, 110, 302, 302]\n",
      "--------------------------------\n",
      "bird  :  96.3  :  [1, 123, 73, 332]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "output_folder= \"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/lecture_notes/lecture_5/output_path\"\n",
=======
    "output_folder= \"/Users/christine/VSCode/MMAI5500_remote/MMAI5500/output_path\"\n",
>>>>>>> d98b390 (Added code.)
    "i = 0\n",
    "\n",
    "for item in fnames:\n",
    "    i = i + 1\n",
    "    output_image_path = os.path.join(output_folder, f\"output_image_{i}.jpg\")\n",
    "    detections = detector.detectObjectsFromImage(input_image=item,\n",
    "                                             output_image_path= output_image_path,\n",
    "                                             minimum_percentage_probability=30)\n",
    "    # Results for each image:\n",
    "    \n",
    "    \n",
    "    for eachObject in detections:\n",
    "        print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghJ6oHyVdIKp"
   },
   "source": [
    "**Tutorial question 1**: How many objects were detected in images 6, 7 and 9?\n",
    "\n",
    "**Tutorial question 2**: What is detected in image 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFr444wSgSgw"
   },
   "outputs": [],
   "source": [
    "# Objects identifed = 1, 1 , 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7oxd8vigrpa"
   },
   "source": [
    "\n",
    "## Task 2\n",
    "Compare the performance of the 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "yTfEHc9JfqIR",
    "outputId": "ac1b3485-248b-4c32-a66a-fe28a3152e86"
   },
   "outputs": [],
   "source": [
    "# Put in a for-loop\n",
    "\n",
    "# Find a list of images, feed into the image one by one, detected with better name\n",
    "# Output will be 1 detected, image 2 will be 2 detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wCZuUocbubg",
    "outputId": "8ef324af-6efb-4824-da8d-e4af6ed563f9"
   },
   "outputs": [],
   "source": [
    "# Which model is best? Any of them, really. But we saw in the Waterfall img that it accidentally deleted a ledge as a person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN-6FQ3ri8NC"
   },
   "source": [
    "**Tutorial question 3**: Which model was best?\n",
    "\n",
    "## Object detection in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZVsKaeWaOaH",
    "outputId": "e405051c-7374-4636-b533-196590e7679a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  tutorial_video.zip\n",
      "  inflating: data/video/traffic.mp4  \n",
      "  inflating: data/video/traffic-mini.mp4  \n",
      "  inflating: data/video/holo1.mp4    \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/video\n",
    "!unzip tutorial_video.zip -d data/video\n",
    "video_path = 'data/video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6gRGb3pojHIo"
   },
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "detector = VideoObjectDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "D4uXa6O1jKYY"
   },
   "outputs": [],
   "source": [
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"yolov3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XJ_jHJkDjOi5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/MMAI5500_remote/MMAI5500/.venv/lib/python3.12/site-packages/imageai/Detection/__init__.py:255: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.__model_path, map_location=self.__device)\n"
     ]
    }
   ],
   "source": [
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8odx7O9jQRw",
    "outputId": "6cda5308-2466-4106-9efe-d1a3fb979bc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame :  1\n",
      "Processing Frame :  2\n",
      "Processing Frame :  3\n",
      "Processing Frame :  4\n",
      "Processing Frame :  5\n",
      "Processing Frame :  6\n",
      "Processing Frame :  7\n",
      "Processing Frame :  8\n",
      "Processing Frame :  9\n",
      "Processing Frame :  10\n",
      "Processing Frame :  11\n",
      "Processing Frame :  12\n",
      "Processing Frame :  13\n",
      "Processing Frame :  14\n",
      "Processing Frame :  15\n",
      "Processing Frame :  16\n",
      "Processing Frame :  17\n",
      "Processing Frame :  18\n",
      "Processing Frame :  19\n",
      "Processing Frame :  20\n",
      "Processing Frame :  21\n",
      "Processing Frame :  22\n",
      "Processing Frame :  23\n",
      "Processing Frame :  24\n",
      "Processing Frame :  25\n",
      "Processing Frame :  26\n",
      "Processing Frame :  27\n",
      "Processing Frame :  28\n",
      "Processing Frame :  29\n",
      "Processing Frame :  30\n",
      "Processing Frame :  31\n",
      "Processing Frame :  32\n",
      "Processing Frame :  33\n",
      "Processing Frame :  34\n",
      "Processing Frame :  35\n",
      "Processing Frame :  36\n",
      "Processing Frame :  37\n",
      "Processing Frame :  38\n",
      "Processing Frame :  39\n",
      "Processing Frame :  40\n",
      "Processing Frame :  41\n",
      "Processing Frame :  42\n",
      "Processing Frame :  43\n",
      "Processing Frame :  44\n",
      "Processing Frame :  45\n",
      "Processing Frame :  46\n",
      "Processing Frame :  47\n",
      "Processing Frame :  48\n",
      "Processing Frame :  49\n",
      "Processing Frame :  50\n",
      "Processing Frame :  51\n",
      "Processing Frame :  52\n",
      "Processing Frame :  53\n",
      "Processing Frame :  54\n",
      "Processing Frame :  55\n",
      "Processing Frame :  56\n",
      "Processing Frame :  57\n",
      "Processing Frame :  58\n",
      "Processing Frame :  59\n",
      "Processing Frame :  60\n",
      "Processing Frame :  61\n",
      "Processing Frame :  62\n",
      "Processing Frame :  63\n",
      "Processing Frame :  64\n",
      "Processing Frame :  65\n",
      "Processing Frame :  66\n",
      "Processing Frame :  67\n",
      "Processing Frame :  68\n",
      "Processing Frame :  69\n",
      "Processing Frame :  70\n",
      "Processing Frame :  71\n",
      "Processing Frame :  72\n",
      "Processing Frame :  73\n",
      "Processing Frame :  74\n",
      "Processing Frame :  75\n",
      "Processing Frame :  76\n",
      "Processing Frame :  77\n",
      "Processing Frame :  78\n",
      "Processing Frame :  79\n",
      "Processing Frame :  80\n",
      "Processing Frame :  81\n",
      "Processing Frame :  82\n",
      "Processing Frame :  83\n",
      "Processing Frame :  84\n",
      "Processing Frame :  85\n",
      "Processing Frame :  86\n",
      "Processing Frame :  87\n",
      "Processing Frame :  88\n",
      "Processing Frame :  89\n",
      "Processing Frame :  90\n",
      "Processing Frame :  91\n",
      "Processing Frame :  92\n",
      "Processing Frame :  93\n",
      "Processing Frame :  94\n",
      "Processing Frame :  95\n",
      "Processing Frame :  96\n",
      "Processing Frame :  97\n",
      "Processing Frame :  98\n",
      "Processing Frame :  99\n",
      "Processing Frame :  100\n",
      "Processing Frame :  101\n",
      "Processing Frame :  102\n",
      "Processing Frame :  103\n",
      "Processing Frame :  104\n",
      "Processing Frame :  105\n",
      "Processing Frame :  106\n",
      "Processing Frame :  107\n",
      "Processing Frame :  108\n",
      "Processing Frame :  109\n",
      "Processing Frame :  110\n",
      "Processing Frame :  111\n",
      "Processing Frame :  112\n",
      "Processing Frame :  113\n",
      "Processing Frame :  114\n",
      "Processing Frame :  115\n",
      "Processing Frame :  116\n",
      "Processing Frame :  117\n",
      "Processing Frame :  118\n",
      "Processing Frame :  119\n",
      "Processing Frame :  120\n",
      "Processing Frame :  121\n",
      "Processing Frame :  122\n",
      "Processing Frame :  123\n",
      "Processing Frame :  124\n",
      "Processing Frame :  125\n",
      "Processing Frame :  126\n",
      "Processing Frame :  127\n",
      "Processing Frame :  128\n",
      "Processing Frame :  129\n",
      "Processing Frame :  130\n",
      "Processing Frame :  131\n",
      "Processing Frame :  132\n",
      "Processing Frame :  133\n",
      "Processing Frame :  134\n",
      "Processing Frame :  135\n",
      "Processing Frame :  136\n",
      "Processing Frame :  137\n",
      "Processing Frame :  138\n",
      "Processing Frame :  139\n",
      "Processing Frame :  140\n",
      "Processing Frame :  141\n",
      "Processing Frame :  142\n",
      "Processing Frame :  143\n",
      "Processing Frame :  144\n",
      "Processing Frame :  145\n",
      "Processing Frame :  146\n",
      "Processing Frame :  147\n",
      "Processing Frame :  148\n",
      "Processing Frame :  149\n",
      "Processing Frame :  150\n",
      "Processing Frame :  151\n",
      "Processing Frame :  152\n",
      "Processing Frame :  153\n",
      "Processing Frame :  154\n",
      "Processing Frame :  155\n",
      "Processing Frame :  156\n",
      "Processing Frame :  157\n",
      "Processing Frame :  158\n",
      "Processing Frame :  159\n",
      "Processing Frame :  160\n",
      "Processing Frame :  161\n",
      "Processing Frame :  162\n",
      "Processing Frame :  163\n",
      "Processing Frame :  164\n",
      "Processing Frame :  165\n",
      "Processing Frame :  166\n",
      "Processing Frame :  167\n",
      "Processing Frame :  168\n",
      "Processing Frame :  169\n",
      "Processing Frame :  170\n",
      "Processing Frame :  171\n",
      "Processing Frame :  172\n",
      "Processing Frame :  173\n",
      "Processing Frame :  174\n",
      "Processing Frame :  175\n",
      "Processing Frame :  176\n",
      "Processing Frame :  177\n",
      "Processing Frame :  178\n",
      "Processing Frame :  179\n",
      "traffic-mini_detected.mp4\n"
     ]
    }
   ],
   "source": [
    "video_path = detector.detectObjectsFromVideo(os.path.join(video_path, \"traffic-mini.mp4\"),\n",
    "                                             output_file_path=\"traffic-mini_detected\",\n",
    "                                             frames_per_second=5, log_progress=True)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj_azW8zjs_5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
